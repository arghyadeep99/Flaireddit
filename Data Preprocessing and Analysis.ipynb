{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import nltk, string\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer, TfidfVectorizer)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix)\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>ID</th>\n",
       "      <th>Upvotes</th>\n",
       "      <th>URL</th>\n",
       "      <th>Num_comments</th>\n",
       "      <th>Creation Date</th>\n",
       "      <th>Body</th>\n",
       "      <th>Is_original</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Flair</th>\n",
       "      <th>Comments_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uber driver scammed me by not picking me up an...</td>\n",
       "      <td>cc6wf1</td>\n",
       "      <td>96</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/cc6wf1...</td>\n",
       "      <td>59</td>\n",
       "      <td>2019-07-12 13:53:45</td>\n",
       "      <td>Yesterday it was raining heavily and needed to...</td>\n",
       "      <td>False</td>\n",
       "      <td>In your case, the driver might have come near ...</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>In your case, the driver might have come near ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assembly Elections Results Megathread</td>\n",
       "      <td>a54j07</td>\n",
       "      <td>277</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/a54j07...</td>\n",
       "      <td>856</td>\n",
       "      <td>2018-12-11 14:58:33</td>\n",
       "      <td>---\\n# Rajasthan\\n\\nTotal Seats: 199, Majority...</td>\n",
       "      <td>False</td>\n",
       "      <td>If elections are not one-sided, it's always go...</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>If elections are not one-sided, it's always go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Travelled to Kashmir 2 years back. This is the...</td>\n",
       "      <td>ekd70o</td>\n",
       "      <td>19</td>\n",
       "      <td>https://www.youtube.com/watch?v=6WgnfARwOdA</td>\n",
       "      <td>7</td>\n",
       "      <td>2020-01-05 21:46:20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>Cringe Good Job brother... It looks good !</td>\n",
       "      <td>Photography</td>\n",
       "      <td>Cringe Good Job brother... It looks good !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reminder to file your Income tax return - 31st...</td>\n",
       "      <td>ct4z8k</td>\n",
       "      <td>59</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/ct4z8k...</td>\n",
       "      <td>21</td>\n",
       "      <td>2019-08-21 03:51:02</td>\n",
       "      <td>Just filing my tax returns, thought of remindi...</td>\n",
       "      <td>False</td>\n",
       "      <td>Hey, thanks! Skipped my mind. 16 years of educ...</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>Hey, thanks! Skipped my mind. 16 years of educ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After eating 900 mice, cat goes on Haj! RJio j...</td>\n",
       "      <td>fgarrj</td>\n",
       "      <td>214</td>\n",
       "      <td>https://www.financialexpress.com/opinion/after...</td>\n",
       "      <td>39</td>\n",
       "      <td>2020-03-10 16:46:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Jio gives free service for six months\\n\\nIndia...</td>\n",
       "      <td>Business/Finance</td>\n",
       "      <td>Jio gives free service for six months\\n\\nIndia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title      ID  Upvotes  \\\n",
       "0  Uber driver scammed me by not picking me up an...  cc6wf1       96   \n",
       "1              Assembly Elections Results Megathread  a54j07      277   \n",
       "2  Travelled to Kashmir 2 years back. This is the...  ekd70o       19   \n",
       "3  Reminder to file your Income tax return - 31st...  ct4z8k       59   \n",
       "4  After eating 900 mice, cat goes on Haj! RJio j...  fgarrj      214   \n",
       "\n",
       "                                                 URL  Num_comments  \\\n",
       "0  https://www.reddit.com/r/india/comments/cc6wf1...            59   \n",
       "1  https://www.reddit.com/r/india/comments/a54j07...           856   \n",
       "2        https://www.youtube.com/watch?v=6WgnfARwOdA             7   \n",
       "3  https://www.reddit.com/r/india/comments/ct4z8k...            21   \n",
       "4  https://www.financialexpress.com/opinion/after...            39   \n",
       "\n",
       "         Creation Date                                               Body  \\\n",
       "0  2019-07-12 13:53:45  Yesterday it was raining heavily and needed to...   \n",
       "1  2018-12-11 14:58:33  ---\\n# Rajasthan\\n\\nTotal Seats: 199, Majority...   \n",
       "2  2020-01-05 21:46:20                                                NaN   \n",
       "3  2019-08-21 03:51:02  Just filing my tax returns, thought of remindi...   \n",
       "4  2020-03-10 16:46:22                                                NaN   \n",
       "\n",
       "   Is_original                                           Comments  \\\n",
       "0        False  In your case, the driver might have come near ...   \n",
       "1        False  If elections are not one-sided, it's always go...   \n",
       "2         True         Cringe Good Job brother... It looks good !   \n",
       "3        False  Hey, thanks! Skipped my mind. 16 years of educ...   \n",
       "4        False  Jio gives free service for six months\\n\\nIndia...   \n",
       "\n",
       "              Flair                                       Comments_new  \n",
       "0     [R]eddiquette  In your case, the driver might have come near ...  \n",
       "1     [R]eddiquette  If elections are not one-sided, it's always go...  \n",
       "2       Photography         Cringe Good Job brother... It looks good !  \n",
       "3     [R]eddiquette  Hey, thanks! Skipped my mind. 16 years of educ...  \n",
       "4  Business/Finance  Jio gives free service for six months\\n\\nIndia...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.drop([\"Unnamed: 0\"], inplace=True, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2894 entries, 0 to 2893\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Title          2894 non-null   object\n",
      " 1   ID             2894 non-null   object\n",
      " 2   Upvotes        2894 non-null   int64 \n",
      " 3   URL            2894 non-null   object\n",
      " 4   Num_comments   2894 non-null   int64 \n",
      " 5   Creation Date  2894 non-null   object\n",
      " 6   Body           1070 non-null   object\n",
      " 7   Is_original    2894 non-null   bool  \n",
      " 8   Comments       2741 non-null   object\n",
      " 9   Flair          2894 non-null   object\n",
      " 10  Comments_new   2741 non-null   object\n",
      "dtypes: bool(1), int64(2), object(8)\n",
      "memory usage: 229.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[R]eddiquette', 'Photography', 'Business/Finance',\n",
       "       'Science/Technology', 'AMA', 'Coronavirus', 'Sports', 'Food',\n",
       "       'Policy/Economy', 'AskIndia', 'Politics', 'Non-Political'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Flair\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uber driver scammed me by not picking me up an...</td>\n",
       "      <td>Yesterday it was raining heavily and needed to...</td>\n",
       "      <td>In your case, the driver might have come near ...</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assembly Elections Results Megathread</td>\n",
       "      <td>---\\n# Rajasthan\\n\\nTotal Seats: 199, Majority...</td>\n",
       "      <td>If elections are not one-sided, it's always go...</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Travelled to Kashmir 2 years back. This is the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cringe Good Job brother... It looks good !</td>\n",
       "      <td>Photography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reminder to file your Income tax return - 31st...</td>\n",
       "      <td>Just filing my tax returns, thought of remindi...</td>\n",
       "      <td>Hey, thanks! Skipped my mind. 16 years of educ...</td>\n",
       "      <td>[R]eddiquette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After eating 900 mice, cat goes on Haj! RJio j...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jio gives free service for six months\\n\\nIndia...</td>\n",
       "      <td>Business/Finance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Uber driver scammed me by not picking me up an...   \n",
       "1              Assembly Elections Results Megathread   \n",
       "2  Travelled to Kashmir 2 years back. This is the...   \n",
       "3  Reminder to file your Income tax return - 31st...   \n",
       "4  After eating 900 mice, cat goes on Haj! RJio j...   \n",
       "\n",
       "                                                Body  \\\n",
       "0  Yesterday it was raining heavily and needed to...   \n",
       "1  ---\\n# Rajasthan\\n\\nTotal Seats: 199, Majority...   \n",
       "2                                                NaN   \n",
       "3  Just filing my tax returns, thought of remindi...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                            Comments             Flair  \n",
       "0  In your case, the driver might have come near ...     [R]eddiquette  \n",
       "1  If elections are not one-sided, it's always go...     [R]eddiquette  \n",
       "2         Cringe Good Job brother... It looks good !       Photography  \n",
       "3  Hey, thanks! Skipped my mind. 16 years of educ...     [R]eddiquette  \n",
       "4  Jio gives free service for six months\\n\\nIndia...  Business/Finance  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_features = [\"Title\", \"Body\", \"Comments\", \"Flair\"]\n",
    "relevant_data = data[relevant_features].copy()\n",
    "relevant_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data['id'] = relevant_data['Flair'].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[R]eddiquette': 0,\n",
       " 'Photography': 1,\n",
       " 'Business/Finance': 2,\n",
       " 'Science/Technology': 3,\n",
       " 'AMA': 4,\n",
       " 'Coronavirus': 5,\n",
       " 'Sports': 6,\n",
       " 'Food': 7,\n",
       " 'Policy/Economy': 8,\n",
       " 'AskIndia': 9,\n",
       " 'Politics': 10,\n",
       " 'Non-Political': 11}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_index, index_flair = {},{}\n",
    "flair_category = relevant_data[['Flair', 'id']].drop_duplicates().sort_values('id')\n",
    "dict_flair = flair_category.to_dict('split')\n",
    "\n",
    "for pair in dict_flair['data']:\n",
    "    flair_index[pair[0]] = pair[1]\n",
    "    index_flair[pair[1]] = pair[0]\n",
    "flair_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b9e61bc2f1d0>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relevant_data[\"Combine\"][i] += ' ' + relevant_data[\"Body\"][i]\n",
      "<ipython-input-9-b9e61bc2f1d0>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relevant_data[\"Combine\"][i] += ' ' + relevant_data[\"Comments\"][i]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Uber driver scammed me by not picking me up an...\n",
       "1    Assembly Elections Results Megathread ---\\n# R...\n",
       "2    Travelled to Kashmir 2 years back. This is the...\n",
       "3    Reminder to file your Income tax return - 31st...\n",
       "4    After eating 900 mice, cat goes on Haj! RJio j...\n",
       "Name: Combine, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_data[\"Combine\"] = relevant_data[\"Title\"].copy()\n",
    "for i in range(len(relevant_data)):\n",
    "    if type(relevant_data.loc[i]['Body']) != float:\n",
    "        relevant_data[\"Combine\"][i] += ' ' + relevant_data[\"Body\"][i]\n",
    "    if type(relevant_data.loc[i]['Comments']) != float:\n",
    "        relevant_data[\"Combine\"][i] += ' ' + relevant_data[\"Comments\"][i]\n",
    "relevant_data[\"Combine\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/arghyadeep99/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "REPLACE_SPACES = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS = re.compile('[^0-9a-z #+_]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = REPLACE_SPACES.sub(' ', text) \n",
    "    text = BAD_SYMBOLS.sub(' ', text)\n",
    "    text = text.replace('x', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data['Combine'] = relevant_data['Combine'].apply(clean_text)\n",
    "relevant_data['Combine'] = relevant_data['Combine'].str.replace('\\d+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bjp leader arrested raping minor   years mumbai bjp need get house order joke many criminals positions disgusting recent court decisions ridiculous bjp must take responsibility pedophile stop calling leader say another bjp pedophile found party members alright guys hold another rally yeah let reuse posters last rally staged support child rapist removed standard becoming member bjp damn putrichod bjp knack recruiting rapist future cm pm beti bachao jeez many rapists party arre kyun pakda bechare ko sala bina matlab victim ke gharwale darr jayenge ki sala saboot na mitane lage ye pradhan mantri ho sake bacche bachao yojana support march coming       raj thackeray theatening mumbaikars come  nd aug summoned enforcement directorate appear interesting week ahead mumbai bois name please seen news tv regarding surprised either removed beti chudvavv office bearer calling bjp leader misleading'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_data['Combine'][1002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/arghyadeep99/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "#remove punctuation, lowercase, stem\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, \n",
    "                             stop_words=STOPWORDS, \n",
    "                             sublinear_tf=True, \n",
    "                             min_df=5, \n",
    "                             norm = 'l2', \n",
    "                             encoding='latin-1', \n",
    "                             ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features by fitting the Vectorizer on Combined Data\n",
    "##labels = relevant_data['id']    # Series containing all the post labels\n",
    "#print(feat.shape)\n",
    "\n",
    "relevant_data[\"norm_combine\"] = relevant_data[\"Combine\"].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arghyadeep99/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hi', 'isnt', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'thi', 'veri', 'wa', 'wasnt', 'werent', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "feat = vectorizer.fit_transform(relevant_data['Combine']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2894, 32467)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = relevant_data['id']\n",
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flair 'AMA':\n",
      "Most correlated unigrams:\n",
      "\t. favorit\n",
      "\t. question\n",
      "\t. hello\n",
      "\t. hi\n",
      "\t. ama\n",
      "Most correlated bigrams:\n",
      "\t. hi thank\n",
      "\t. ama question\n",
      "\t. answer question\n",
      "\t. ask anyth\n",
      "\t. thank ama\n",
      "\n",
      "Flair 'AskIndia':\n",
      "Most correlated unigrams:\n",
      "\t. linkedin\n",
      "\t. famili\n",
      "\t. parent\n",
      "\t. relationship\n",
      "\t. depend\n",
      "Most correlated bigrams:\n",
      "\t. drive car\n",
      "\t. irrespect age\n",
      "\t. easi get\n",
      "\t. get marri\n",
      "\t. tier colleg\n",
      "\n",
      "Flair 'Business/Finance':\n",
      "Most correlated unigrams:\n",
      "\t. payment\n",
      "\t. jio\n",
      "\t. ambani\n",
      "\t. relianc\n",
      "\t. bank\n",
      "Most correlated bigrams:\n",
      "\t. invest rs\n",
      "\t. vodafon idea\n",
      "\t. anil ambani\n",
      "\t. ye bank\n",
      "\t. jio platform\n",
      "\n",
      "Flair 'Coronavirus':\n",
      "Most correlated unigrams:\n",
      "\t. test\n",
      "\t. patient\n",
      "\t. hospit\n",
      "\t. coronaviru\n",
      "\t. covid\n",
      "Most correlated bigrams:\n",
      "\t. posit covid\n",
      "\t. news news\n",
      "\t. covid case\n",
      "\t. test posit\n",
      "\t. coronaviru case\n",
      "\n",
      "Flair 'Food':\n",
      "Most correlated unigrams:\n",
      "\t. biryani\n",
      "\t. chutney\n",
      "\t. chicken\n",
      "\t. delici\n",
      "\t. recip\n",
      "Most correlated bigrams:\n",
      "\t. look yummi\n",
      "\t. look tasti\n",
      "\t. share recip\n",
      "\t. ice cream\n",
      "\t. look delici\n",
      "\n",
      "Flair 'Non-Political':\n",
      "Most correlated unigrams:\n",
      "\t. fairer\n",
      "\t. skin\n",
      "\t. pineappl\n",
      "\t. locker\n",
      "\t. irrfan\n",
      "Most correlated bigrams:\n",
      "\t. compl ion\n",
      "\t. fair skin\n",
      "\t. dark skin\n",
      "\t. irrfan khan\n",
      "\t. locker room\n",
      "\n",
      "Flair 'Photography':\n",
      "Most correlated unigrams:\n",
      "\t. mm\n",
      "\t. beauti\n",
      "\t. shot\n",
      "\t. nikon\n",
      "\t. oc\n",
      "Most correlated bigrams:\n",
      "\t. mm f\n",
      "\t. one plu\n",
      "\t. f iso\n",
      "\t. equip nikon\n",
      "\t. nikon mm\n",
      "\n",
      "Flair 'Policy/Economy':\n",
      "Most correlated unigrams:\n",
      "\t. nirmala\n",
      "\t. sitharaman\n",
      "\t. economi\n",
      "\t. econom\n",
      "\t. gdp\n",
      "Most correlated bigrams:\n",
      "\t. http internetfreedom\n",
      "\t. stimulu packag\n",
      "\t. labour law\n",
      "\t. nirmala sitharaman\n",
      "\t. econom packag\n",
      "\n",
      "Flair 'Politics':\n",
      "Most correlated unigrams:\n",
      "\t. elect\n",
      "\t. bhakt\n",
      "\t. muslim\n",
      "\t. congress\n",
      "\t. bjp\n",
      "Most correlated bigrams:\n",
      "\t. suprem court\n",
      "\t. kapil mishra\n",
      "\t. georg floyd\n",
      "\t. pm care\n",
      "\t. manoj tiwari\n",
      "\n",
      "Flair 'Science/Technology':\n",
      "Most correlated unigrams:\n",
      "\t. bhim\n",
      "\t. zoom\n",
      "\t. android\n",
      "\t. data\n",
      "\t. app\n",
      "Most correlated bigrams:\n",
      "\t. data breach\n",
      "\t. data scientist\n",
      "\t. isro get\n",
      "\t. bhim app\n",
      "\t. aarogya setu\n",
      "\n",
      "Flair 'Sports':\n",
      "Most correlated unigrams:\n",
      "\t. chess\n",
      "\t. championship\n",
      "\t. footbal\n",
      "\t. sport\n",
      "\t. cricket\n",
      "Most correlated bigrams:\n",
      "\t. bo ing\n",
      "\t. usain bolt\n",
      "\t. indian cricket\n",
      "\t. sachin tendulkar\n",
      "\t. world cup\n",
      "\n",
      "Flair '[R]eddiquette':\n",
      "Most correlated unigrams:\n",
      "\t. rape\n",
      "\t. beep\n",
      "\t. unnao\n",
      "\t. askaway\n",
      "\t. boop\n",
      "Most correlated bigrams:\n",
      "\t. bot problem\n",
      "\t. askaway creator\n",
      "\t. problem askaway\n",
      "\t. beep boop\n",
      "\t. discuss thread\n"
     ]
    }
   ],
   "source": [
    "N = 5    # Number of examples to be listed\n",
    "for f, i in sorted(flair_index.items()):\n",
    "    chi2_feat = chi2(feat, labels == i)\n",
    "    indices = np.argsort(chi2_feat[0])\n",
    "    feat_names = np.array(vectorizer.get_feature_names())[indices]\n",
    "    unigrams = [w for w in feat_names if len(w.split(' ')) == 1]\n",
    "    bigrams = [w for w in feat_names if len(w.split(' ')) == 2]\n",
    "    print(\"\\nFlair '{}':\".format(f))\n",
    "    print(\"Most correlated unigrams:\\n\\t. {}\".format('\\n\\t. '.join(unigrams[-N:])))\n",
    "    print(\"Most correlated bigrams:\\n\\t. {}\".format('\\n\\t. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2315,) (2315,) (579,) (579,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(relevant_data['Combine'], relevant_data['Flair'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=97)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "X_train_counts = count_vec.fit_transform(X_train)\n",
    "\n",
    "# Creating an instance of the TFID transformer\n",
    "tfidf_trans = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_trans.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    nb_fit = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', MultinomialNB()),\n",
    "                 ])\n",
    "    nb_fit.fit(X_train, y_train)    # Fitting the data to the trianing data\n",
    "    \n",
    "    # Making Predictions on the test data\n",
    "    y_pred = nb_fit.predict(X_test)\n",
    "    acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    print(\"Model Accuracy: {}\".format(acc))\n",
    "    return nb_fit\n",
    "    \n",
    "def random_forest(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    forest = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', RandomForestClassifier()),\n",
    "                 ])\n",
    "    forest.fit(X_train, y_train)    # Fitting the data to the trianing data\n",
    "    \n",
    "    # Making Predictions on the test data\n",
    "    y_pred = forest.predict(X_test)\n",
    "    acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    print(\"Model Accuracy: {}\".format(acc))\n",
    "    return forest\n",
    "\n",
    "# Support Vector Machines Classifier \n",
    "def svc(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    svc_fit = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', SVC(C=1, gamma=1, kernel='linear')),\n",
    "                 ])\n",
    "    svc_fit.fit(X_train, y_train)    # Fitting the data to the trianing data\n",
    "    \n",
    "    # Making Predictions on the test data\n",
    "    y_pred = svc_fit.predict(X_test)\n",
    "    acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    print(\"Model Accuracy: {}\".format(acc))\n",
    "    return svc_fit\n",
    "\n",
    "# Logistic Regression Classifier \n",
    "def log_reg(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('model', LogisticRegression()),\n",
    "                 ])\n",
    "    logreg.fit(X_train, y_train)     # Fitting the data to the trianing data\n",
    "\n",
    "    # Making Predictions on the test data\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "    print(\"Model Accuracy: {}\".format(acc))\n",
    "    return logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.4835924006908463\n",
      "Model Accuracy: 0.5267702936096719\n",
      "Model Accuracy: 0.6424870466321243\n",
      "Model Accuracy: 0.613126079447323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('model', LogisticRegression())])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes(X_train, X_test, y_train, y_test)\n",
    "random_forest(X_train, X_test, y_train, y_test)\n",
    "svc(X_train, X_test, y_train, y_test)\n",
    "log_reg(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, SVM is performing the best. So, I will save this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.6424870466321243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_svm.sav']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "model = svc(X_train, X_test, y_train, y_test)\n",
    "model.fit(X_train, y_train)\n",
    "joblib.dump(model, 'final_svm.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-09 11:35:56--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.174.240, 2404:6800:4009:801::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.174.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 407727028 (389M) [application/zip]\n",
      "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
      "\n",
      "uncased_L-12_H-768_ 100%[===================>] 388.84M  2.93MB/s    in 2m 14s  \n",
      "\n",
      "2020-06-09 11:38:11 (2.90 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
      "\n",
      "--2020-06-09 11:38:11--  https://raw.githubusercontent.com/google-research/bert/master/modeling.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 37922 (37K) [text/plain]\n",
      "Saving to: ‘modeling.py’\n",
      "\n",
      "modeling.py         100%[===================>]  37.03K  --.-KB/s    in 0.007s  \n",
      "\n",
      "2020-06-09 11:38:12 (4.93 MB/s) - ‘modeling.py’ saved [37922/37922]\n",
      "\n",
      "--2020-06-09 11:38:12--  https://raw.githubusercontent.com/google-research/bert/master/optimization.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.128.133, 151.101.64.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6258 (6.1K) [text/plain]\n",
      "Saving to: ‘optimization.py’\n",
      "\n",
      "optimization.py     100%[===================>]   6.11K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-06-09 11:38:12 (40.0 MB/s) - ‘optimization.py’ saved [6258/6258]\n",
      "\n",
      "--2020-06-09 11:38:12--  https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.192.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34783 (34K) [text/plain]\n",
      "Saving to: ‘run_classifier.py’\n",
      "\n",
      "run_classifier.py   100%[===================>]  33.97K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2020-06-09 11:38:13 (4.03 MB/s) - ‘run_classifier.py’ saved [34783/34783]\n",
      "\n",
      "--2020-06-09 11:38:13--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.64.133, 151.101.0.133, 151.101.192.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.64.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12257 (12K) [text/plain]\n",
      "Saving to: ‘tokenization.py’\n",
      "\n",
      "tokenization.py     100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-06-09 11:38:13 (28.4 MB/s) - ‘tokenization.py’ saved [12257/12257]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n",
    "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'Optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-ee198823909e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m train_InputExamples = X_train.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bert/run_classifier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bert/optimization.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAdamWeightDecayOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m   \u001b[0;34m\"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "train_InputExamples = X_train.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
    "                                                                   text_a = x['Combine'], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x['id']))\n",
    "val_InputExamples = X_test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x['Combine'], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return bert.tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# Convert our train and validation features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "val_features = bert.run_classifier.convert_examples_to_features(val_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
